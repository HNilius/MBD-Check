[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A decision support tool to improve the preoperative screening of mild bleeding disorders: A cross-sectional model development, external validation, and user-centric evaluation study",
    "section": "",
    "text": "Introduction\n\n\n\n\n\nThis ebook containt the code for the manuscript.The anonymised dataset can be obtained from the corresponding author upon reasonable request."
  },
  {
    "objectID": "Featureselection.html#introduction",
    "href": "Featureselection.html#introduction",
    "title": "1  Feature selection",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nIn this chapter the feature selection for the MBD Check Algorithm is described. First a focus group decided on the most relevant predictors that are reasonably available in the preoperative setting. Then the Boruta algorithm was used to select the most significant predictors (Kursa and Rudnicki 2010). Results can be seen in Figure 1.1 and the imporatance of significant predictors in the first table."
  },
  {
    "objectID": "Featureselection.html#data-loading-imputation",
    "href": "Featureselection.html#data-loading-imputation",
    "title": "1  Feature selection",
    "section": "1.2 Data loading & imputation",
    "text": "1.2 Data loading & imputation\nThe trainings data is loaded for feature importance calculation.\n\nble.tr &lt;- readRDS(\"TrainingsSet.RDS\")\n\nLoading required libraries:\n\nlibrary(Boruta) # Boruta algorithm\nlibrary(ggplot2) # Visualization\nlibrary(missForest) # Imputation\nlibrary(DT)\n\nSince the Boruta algorithm can not handle incomplete data, the data set is imputed.\n\nset.seed(123213)\nimputation.res &lt;- missForest(ble.tr)\nble.tr.imp &lt;- imputation.res$ximp\nprint(imputation.res$OOBerror)\n\n    NRMSE       PFC \n0.3410852 0.1201472"
  },
  {
    "objectID": "Featureselection.html#feature-selection",
    "href": "Featureselection.html#feature-selection",
    "title": "1  Feature selection",
    "section": "1.3 Feature selection",
    "text": "1.3 Feature selection\nAll variables that are not reasonably available at preoperative screening are removed.\n\nset.seed(12312)\nboruta_output &lt;- Boruta(mbd ~ ., data= ble.tr.imp\n                        [,-c(26,9,10,11,12,15,16,17,19,20:22,28,33,42,43,44)], \n                        doTrace=0) \nplot(boruta_output, cex.axis=.7, las=2, xlab=\"\", main=\"Variable Importance\")\n\n\n\n\nFigure 1.1: Output of the Boruta model\n\n\n\n\nTable of the significant predictors:\n\nboruta_signif &lt;- getSelectedAttributes(boruta_output, withTentative = TRUE)\nroughFixMod &lt;- TentativeRoughFix(boruta_output)\nboruta_signif &lt;- getSelectedAttributes(roughFixMod)\nimps &lt;- attStats(roughFixMod)\nimps2 &lt;- imps[imps$decision != 'Rejected', c('meanImp', 'decision')]\nimps2$Predictor &lt;- rownames(imps2)\nimps2 &lt;- imps2[,c(3,1,2)]\nimps2$meanImp &lt;- round(as.numeric(imps2$meanImp),2)\ncolnames(imps2) &lt;- c(\"Predictor\", \"Mean importance\", \"Decision\")\ndatatable(imps2, rownames = FALSE, options = list(order = list(1,\"desc\")))"
  },
  {
    "objectID": "Featureselection.html#final-predictors",
    "href": "Featureselection.html#final-predictors",
    "title": "1  Feature selection",
    "section": "1.4 Final predictors",
    "text": "1.4 Final predictors\nThe final predictors that were selected were: PFA-EPI, Simplified ISTH-BAT: Surgery, PFA-ADP, Simplified ISTH-BAT: Menorrhagia, Simplified ISTH-BAT: Epistaxis, Simplified ISTH-BAT: Minor wounds, Simplified ISTH-BAT: Tooth extraction, aPTT, Simplified ISTH-BAT: Cutaneous bleedings, Simplified ISTH-BAT: Oral cavity bleedings, and Simplified ISTH-BAT: Post-partum hemorrhage.\n\n\n\n\nKursa, Miron B., and Witold R. Rudnicki. 2010. “Feature Selection with the Boruta Package.” Journal of Statistical Software 36 (September): 1–13. https://doi.org/10.18637/jss.v036.i11."
  },
  {
    "objectID": "Modelscreening.html#introduction",
    "href": "Modelscreening.html#introduction",
    "title": "2  Model screening",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nIn this chapter the model screening procedures are described. Six-teen different models were screened. The five best performing models were further tuned. When models with similar architecture were in the top 5, only the best performing model was selected for further tuning."
  },
  {
    "objectID": "Modelscreening.html#data-loading",
    "href": "Modelscreening.html#data-loading",
    "title": "2  Model screening",
    "section": "2.2 Data loading",
    "text": "2.2 Data loading\nThe trainings data is loaded for feature importance calculation.\n\nble.tr &lt;- readRDS(\"TrainingsSet.RDS\")\n\nLoading required libraries:\n\nlibrary(caret) #  Model training\nlibrary(ggplot2) # Visualization\nlibrary(DT) # Data tables\nlibrary(doParallel) # Parallelization\nlibrary(dplyr) # Data manipulation\nlibrary(tidyverse) # Data manipulation\nlibrary(hrbrthemes) # Theme for ggplot\nlibrary(RColorBrewer)"
  },
  {
    "objectID": "Modelscreening.html#model-screening",
    "href": "Modelscreening.html#model-screening",
    "title": "2  Model screening",
    "section": "2.3 Model screening",
    "text": "2.3 Model screening\nA five-fold ten times repeated cross-validation was performed for each model. The semi-proper scoring rule ROC-AUC was chosen as the scoring rule.\n\ntraincontrol3 &lt;-trainControl(method = \"repeatedcv\", number = 10, repeats = 5, allowParallel = TRUE,\n                             summaryFunction = twoClassSummary, selectionFunction = \"best\", classProbs = TRUE)\n\nAll models were preprocessed using bagged-tree based imputation, centering, scaling and a Yeo-Johnson transformation.\n\n2.3.1 Logistic regression based models\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.log &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n              data = ble.tr, trControl = traincontrol3, method = \"glm\", family = \"binomial\", preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n              , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.enlog&lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                data = ble.tr, trControl = traincontrol3, method = \"glmnet\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\n\n\n2.3.2 Tree based models\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.rpart &lt;-  train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                    data = ble.tr, trControl = traincontrol3, method = \"rpart\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                    , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.rf &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                data = ble.tr, trControl = traincontrol3, method = \"rf\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.gbm &lt;- train(mbd ~PFA_EPI + surgery.simp + minorwound.simp + menorrhagia.simp + epistaxis.simp + tooth.simp  + aPTT+ cutaeous.simp +sex,\n                 trControl = traincontrol3,\n                 metric = \"ROC\", data = ble.tr, method = \"gbm\",\n                 preProc = c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\"), verbose = FALSE, na.action = na.pass)\nstopCluster(cls)\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.c5 &lt;- train(mbd ~PFA_EPI + surgery.simp + minorwound.simp + menorrhagia.simp + epistaxis.simp + tooth.simp  + aPTT+ cutaeous.simp +sex,\n                  trControl = traincontrol3,\n                  metric = \"ROC\", data = ble.tr, method = \"C5.0\",\n                  preProc = c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\"), verbose = FALSE, na.action = na.pass)\nstopCluster(cls)\n\n\n\n2.3.3 SVM based models\n\n# SVM \ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.svmlin &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                    data = ble.tr, trControl = traincontrol3, method = \"svmLinear\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                    , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.svmrad &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                    data = ble.tr, trControl = traincontrol3, method = \"svmRadial\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                    , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.svmpoly &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                     data = ble.tr, trControl = traincontrol3, method = \"svmPoly\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                     , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\n\n\n2.3.4 XGB models\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.xgblin &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                    data = ble.tr, trControl = traincontrol3, method = \"xgbLinear\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                    , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.xgbdart &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                    data = ble.tr, trControl = traincontrol3, method = \"xgbDART\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                    , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\n\n\n2.3.5 Neural network based models\n\ncls = makeCluster(8)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.mlp &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                     data = ble.tr, trControl = traincontrol3, method = \"mlp\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                     , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\ncls = makeCluster(8)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.avNNet &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                 data = ble.tr, trControl = traincontrol3, method = \"avNNet\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                 , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.nnet &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                             data = ble.tr, trControl = traincontrol3, method = \"nnet\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                             , metric = \"ROC\", na.action = na.pass, verbose = FALSE)\n\n# weights:  58\ninitial  value 322.659951 \niter  10 value 212.016564\niter  20 value 199.544073\niter  30 value 196.255201\niter  40 value 191.916277\niter  50 value 190.703381\niter  60 value 190.247428\niter  70 value 190.111914\niter  80 value 190.104541\nfinal  value 190.104366 \nconverged\n\nstopCluster(cls)\n\n\n\n2.3.6 Other models\n\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.nb &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                data = ble.tr, trControl = traincontrol3, method = \"nb\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.pda &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                 data = ble.tr, trControl = traincontrol3, method = \"pda\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                 , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nmrf.lda &lt;- train(mbd ~ PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                 data = ble.tr, trControl = traincontrol3, method = \"lda\",  preProc =c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\")\n                 , metric = \"ROC\", na.action = na.pass)\nstopCluster(cls)\n\n\n\n2.3.7 Overview of the models\n\n# Resamples\nlist.rfm &lt;- list(\"Logistic regression\" = mrf.log, \n                 \"Elastic net logistic regression\" = mrf.enlog, \n                 \"Rpart\" = mrf.rpart,\n                 \"Random forest\" = mrf.rf,\n                 \"C5.0\" = mrf.c5,\n                 \"Gradient boosting\" = mrf.gbm,\n                 \"SVM Linear\" = mrf.svmlin,\n                 \"SVM Radial\" = mrf.svmrad,\n                 \"SVM Poly\" = mrf.svmpoly,\n                 \"Naive bayes\" = mrf.nb,\n                 \"PDA\" = mrf.pda,\n                 \"LDA\" = mrf.lda,\n                 \"Neural network\" = mrf.nnet,\n                 \"XGB Linear\" = mrf.xgblin,\n                 \"XGB DART\" = mrf.xgbdart,\n                 \"MLP\" = mrf.mlp,\n                 \"avNNet\" = mrf.avNNet)\nresamples.rfm &lt;- caret::resamples(list.rfm)\ndf.rfm &lt;- as.data.frame(resamples.rfm)\n\n\npiv.rfm &lt;- df.rfm %&gt;%\n  pivot_longer(!Resample,names_to = \"Modeltype\", values_to = \"AUC\") \n\nn &lt;- 60\nqual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]\ncol_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))\n\nggplot(piv.rfm, aes(x = reorder(Modeltype,AUC,FUN = median), y = AUC, fill = Modeltype)) +\n  geom_boxplot() +\n  scale_fill_manual(values = sample(col_vector,17)) +\n  theme_ipsum()+\n  xlab(\"\")+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+\n  theme(\n    panel.background = element_rect(fill = \"white\", color = \"white\"),\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "Modelscreening.html#final-models",
    "href": "Modelscreening.html#final-models",
    "title": "2  Model screening",
    "section": "2.4 Final models",
    "text": "2.4 Final models\nThe following models were further tuned: Random forest, SVM with a radial kernel, avNNet, XGB Dart and elastic net logistic regression."
  },
  {
    "objectID": "Modeltraining.html#introduction",
    "href": "Modeltraining.html#introduction",
    "title": "3  Model training",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nThis chapter shows the training of the five best performing models in the model screening and the internal validation. Hyperparameter tuning is not shown."
  },
  {
    "objectID": "Modeltraining.html#data-loading",
    "href": "Modeltraining.html#data-loading",
    "title": "3  Model training",
    "section": "3.2 Data loading",
    "text": "3.2 Data loading\nThe data sets are loaded:\n\nble.tr &lt;- readRDS(\"TrainingsSet.RDS\")\nble.te &lt;- readRDS(\"InternalValidationSet.RDS\")\n\nLibraries\n\nlibrary(caret) #  Model training\nlibrary(ggplot2) # Visualization\nlibrary(DT) # Data tables\nlibrary(doParallel) # Parallelization\nlibrary(dplyr) # Data manipulation\nlibrary(tidyverse) # Data manipulation\nlibrary(hrbrthemes) # Theme for ggplot\nlibrary(RColorBrewer)\nlibrary(boot)\nlibrary(pROC)\nlibrary(ThresholdROC)"
  },
  {
    "objectID": "Modeltraining.html#model-training",
    "href": "Modeltraining.html#model-training",
    "title": "3  Model training",
    "section": "3.3 Model training",
    "text": "3.3 Model training\n\n#Train control\ntraincontrol3 &lt;-trainControl(method = \"repeatedcv\", number = 10, repeats = 5, allowParallel = TRUE,\n                             summaryFunction = twoClassSummary, selectionFunction = \"best\", classProbs = TRUE)\n\n# Elastic net logistic regression -----------------------------------------------------------------------\nset.seed(1033)\ncls = makeCluster(8)\nregisterDoParallel(cls)\nm.enlog &lt;- train(mbd ~ sex + PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n                 trControl = traincontrol3,\n                 tuneGrid = expand.grid(alpha = 0.3, lambda = 0.01),   metric = \"ROC\", data = ble.tr, method = \"glmnet\",\n                 preProc = c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\"), na.action = na.pass)\nstopCluster(cls)\n\n# avNNet ---------------------------------------------------------------------------------------------------\nset.seed(1033)\ncls = makeCluster(5)\nregisterDoParallel(cls)\nm.avnnet &lt;- train(mbd ~ sex + PFA_EPI + surgery.simp + minorwound.simp + menorrhagia.simp + epistaxis.simp + tooth.simp  + aPTT+ cutaeous.simp +sex,\n               trControl = traincontrol3,\n               tuneGrid = expand.grid(size = c(4),decay = 0.0099, bag = c(FALSE)),  \n               metric = \"ROC\", data = ble.tr, method = \"avNNet\",\n               preProc = c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\"), verbose = FALSE, na.action = na.pass)\nstopCluster(cls)\n\n\n\n# RF ----------------------------------------------------------------------------------------------------\nset.seed(1033)\ncls = makeCluster(5)\nregisterDoParallel(cls)\nset.seed(1033)\nm.rf &lt;- train(mbd ~ sex + PFA_EPI +  surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n              trControl = traincontrol3,\n              tuneGrid = expand.grid(mtry =  3),   \n              metric = \"ROC\", data = ble.tr, method = \"rf\", ntree = 150,\n              preProc = c(\"bagImpute\",\"center\",\"scale\", \"YeoJohnson\"), na.action = na.pass)\nstopCluster(cls)\n\n\n\n# SVM ---------------------------------------------------------------------------------------------------\nset.seed(1033)\ncls = makeCluster(8)\nregisterDoParallel(cls)\nm.svm &lt;- train(mbd ~  sex + PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n               trControl = traincontrol3,\n               metric = \"ROC\", data = ble.tr, method = \"svmRadial\",\n               tuneGrid = expand.grid(C =seq(0.27) ,sigma = 0.0175),\n               preProc = c(\"bagImpute\", \"center\",\"scale\", \"YeoJohnson\"), na.action = na.pass)\nstopCluster(cls)\n\n\n# XGB ---------------------------------------------------------------------------------------------------\nset.seed(1033)\ncls = makeCluster(8)\nregisterDoParallel(cls)\nm.xgb &lt;- train(mbd ~ sex + PFA_EPI + surgery.simp +  menorrhagia.simp + postpartum.simp + minorwound.simp + epistaxis.simp + tooth.simp  + oralcavity.simp+  aPTT+ cutaeous.simp,\n               trControl = traincontrol3, tuneGrid = expand.grid(gamma = 0, min_child_weight = 1,rate_drop =0.01, skip_drop = c(0.05),subsample = c(1), colsample_bytree = 0.8, nrounds = 58, eta =.42, max_depth = c(1) ),\n               maximize = FALSE, data = ble.tr, method = \"xgbDART\",metric = \"ROC\",\n               preProc = c(\"bagImpute\", \"center\",\"scale\", \"YeoJohnson\"), na.action = na.pass)\nstopCluster(cls)"
  },
  {
    "objectID": "Modeltraining.html#performance-in-the-trainings-data-set",
    "href": "Modeltraining.html#performance-in-the-trainings-data-set",
    "title": "3  Model training",
    "section": "3.4 Performance in the trainings data set",
    "text": "3.4 Performance in the trainings data set\n\nn &lt;- 60\nqual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]\ncol_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))\n\n\nmodellist &lt;- list( \"Elastic-net logistic regression\" = m.enlog, \n                  \"avNNet\" = m.avnnet, \"Random forest\" = m.rf, \"Support vector machine\" = m.svm,\n                  \"XGB\" = m.xgb)\nresamp &lt;- resamples(modellist)\ndatresamp &lt;- as.data.frame(resamp)\ntest &lt;- datresamp %&gt;%\n  pivot_longer(!Resample,names_to = \"Modeltype\", values_to = \"AUC\")\nggplot(test,aes(x = Modeltype, y = AUC, fill = Modeltype)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Spectral\")+\n  theme(legend.position = \"none\") +\n  xlab(\"\")+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+\n  theme(\n    panel.background = element_rect(fill = \"white\", color = \"white\"),\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\n    panel.grid.major =  element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\nFigure 3.1: Resample performance during cross-validation"
  },
  {
    "objectID": "Modeltraining.html#internal-validation",
    "href": "Modeltraining.html#internal-validation",
    "title": "3  Model training",
    "section": "3.5 Internal validation",
    "text": "3.5 Internal validation\n\ncalc_roc &lt;- function(df,i, model){\n  x &lt;- df[i,]\n  preds_mod &lt;- predict(model, x, type = \"prob\", na.action = na.pass)[,2]\n  roc_temp &lt;- roc(x$mbd,preds_mod, quiet = TRUE)\n  return(roc_temp$auc)\n}\n\nset.seed(123)\nenlog.boot &lt;- boot(ble.te,calc_roc, R= 10000,stype = \"i\", model = m.enlog)\nset.seed(123)\navnnet.boot &lt;- boot(ble.te,calc_roc, R= 10000,stype = \"i\", model = m.avnnet)\nset.seed(123)\nrf.boot &lt;- boot(ble.te,calc_roc, R= 10000,stype = \"i\", model = m.rf)\nset.seed(123)\n\nsvm.boot &lt;- boot(ble.te,calc_roc, R= 10000,stype = \"i\", model = m.svm)\n\n\ncomb &lt;- data.frame(\"Elastic-net logistic regression\" = enlog.boot$t, \n                   \"avNNet\" = avnnet.boot$t,\n                   \"Random forest\" = rf.boot$t,\n                   \"Support vector machine\"= svm.boot$t,\n                   \"XGB\" = xgb.boot$t)\n\naurocs &lt;- comb %&gt;%\n  pivot_longer(everything(),values_to = \"AUROC\", names_to = \"Model\")\n\nggplot(aurocs, aes(x = reorder(Model,AUROC,FUN = median), y = AUROC, fill = Model)) +\n  geom_boxplot() +\n  scale_fill_manual(values = sample(col_vector,5)) +\n  theme_ipsum()+\n  xlab(\"\")+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+\n  theme(\n    panel.background = element_rect(fill = \"white\", color = \"white\"),\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\n    panel.grid.major =  element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\nFigure 3.2: Performance in the internal validation set after 10000 bootstraps. The model with the best median performance was selected."
  },
  {
    "objectID": "Modeltraining.html#final-model",
    "href": "Modeltraining.html#final-model",
    "title": "3  Model training",
    "section": "3.6 Final model",
    "text": "3.6 Final model\nThe final model that was selected was a support vector machine with a radial kernel. Using bootstraps the median AUROC in the internal validation set was 0.75 (95% CI: 0.65, 0.84)."
  },
  {
    "objectID": "Modeltraining.html#cut-off-selection",
    "href": "Modeltraining.html#cut-off-selection",
    "title": "3  Model training",
    "section": "3.7 Cut-off selection",
    "text": "3.7 Cut-off selection\n\np.te.svm &lt;- predict(m.svm, ble.te, type = \"prob\", na.action = na.pass)[,2]\nth &lt;- thres2(p.te.svm[ble.te$mbd == \"NoMBD\"],p.te.svm[ble.te$mbd == \"MBD\"], rho = 0.5, \n       costs = matrix(c(0, 0, 1, 1.5), 2, 2, byrow = TRUE))\nplot(th)\n\n\n\n\nFigure 3.3: Cost-based estimation of the optimal threshold.\n\n\n\n\nThe optimal cut-off in the interal validation set is at 0.39 using a higher cost for false negative patients."
  },
  {
    "objectID": "External_validation.html#introduction",
    "href": "External_validation.html#introduction",
    "title": "4  External validation",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn this chapter the external validation of the model. The model was validated with 217 patients from another tertiary hospital in Switzerland"
  },
  {
    "objectID": "External_validation.html#data-loading",
    "href": "External_validation.html#data-loading",
    "title": "4  External validation",
    "section": "4.2 Data loading",
    "text": "4.2 Data loading\nThe data sets are loaded:\n\nlibrary(epiR)\nlibrary(caret)\nlibrary(pROC)\nlibrary(PRROC)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(probably)\n\nm.svm &lt;- readRDS(\"modelsvm.RDS\")\nval.set &lt;- readRDS(\"ExternalValidationSet.RDS\")"
  },
  {
    "objectID": "External_validation.html#diagnostic-accuracy",
    "href": "External_validation.html#diagnostic-accuracy",
    "title": "4  External validation",
    "section": "4.3 Diagnostic accuracy",
    "text": "4.3 Diagnostic accuracy\nUsing the threshhold described in the last document the performance of the model was calculated using the epiR package\n\np.svm &lt;- predict(m.svm, val.set, type = \"prob\", na.action = na.pass)[,2]\np.svm.categorical &lt;- as.factor(ifelse(p.svm &gt; 0.39, \"MBD\", \"NoMBD\"))\ncrosstable &lt;- confusionMatrix(p.svm.categorical, val.set$mbd, positive = \"MBD\")\n\ntest.performance &lt;- epi.tests(crosstable$table[c(2,1),c(2,1)])\ntest.performance\n\n          Outcome +    Outcome -      Total\nTest +           98           48        146\nTest -           14           57         71\nTotal           112          105        217\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.67 (0.61, 0.73)\nTrue prevalence *                      0.52 (0.45, 0.58)\nSensitivity *                          0.88 (0.80, 0.93)\nSpecificity *                          0.54 (0.44, 0.64)\nPositive predictive value *            0.67 (0.59, 0.75)\nNegative predictive value *            0.80 (0.69, 0.89)\nPositive likelihood ratio              1.91 (1.54, 2.38)\nNegative likelihood ratio              0.23 (0.14, 0.39)\nFalse T+ proportion for true D- *      0.46 (0.36, 0.56)\nFalse T- proportion for true D+ *      0.12 (0.07, 0.20)\nFalse T+ proportion for T+ *           0.33 (0.25, 0.41)\nFalse T- proportion for T- *           0.20 (0.11, 0.31)\nCorrectly classified proportion *      0.71 (0.65, 0.77)\n--------------------------------------------------------------\n* Exact CIs"
  },
  {
    "objectID": "External_validation.html#roc-analysis-precision-recall-curves",
    "href": "External_validation.html#roc-analysis-precision-recall-curves",
    "title": "4  External validation",
    "section": "4.4 ROC Analysis & Precision-recall-curves",
    "text": "4.4 ROC Analysis & Precision-recall-curves\nWe compared the performance of our model to the currently available screening tools (PFA-200, ISTH-BAT and INR)\n\n# ROC analysis -----------------------------------------------------------------\nr.svm &lt;- roc(val.set$mbd, p.svm)\nr.isth &lt;- roc(val.set$mbd, val.set$isth2)\nr.pfa &lt;- roc(val.set$mbd, val.set$PFA_EPI)\nr.inr &lt;- roc(val.set$mbd, val.set$INR)\n\nrocble &lt;- ggroc(list(r.svm,r.isth,r.inr,r.pfa), size = 1.3)+\n  theme_pubr()+\n  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color=\"black\", linetype=\"dashed\")+\n  labs(x= \"1 - Specificity\", y = \"Sensitivity\", title = \"Receiver operating characteristic curve\", color = \"Models\")+\n  scale_color_manual(labels = c(\"MBD Check\", \"ISTH-BAT\",  \n                                \"INR\", \"PFA-EPI\"), values = c(\"#019970\",\"#666666\", \"#d6002a\",\"#5990b6\"))+\n  annotate(\"text\", x = 0.35, y = 0.15, size= 5, label = paste(\"INR AUC:\", round(r.inr$auc,2)), hjust = 0) +\n  annotate(\"text\", x = 0.35, y = 0.3, size= 5, label = paste(\"MBD Check AUC:\", round(r.svm$auc,2)), hjust = 0)+\n  annotate(\"text\", x = 0.35, y = 0.25, size= 5, label = paste(\"ISTH-BAT AUC:\", round(r.isth$auc,2)), hjust = 0)+\n  annotate(\"text\", x = 0.35, y = 0.2, size= 5, label = paste(\"PFA-EPI AUC:\", round(r.pfa$auc,2)), hjust = 0) +\n  coord_cartesian(expand = FALSE)\n\n\n## Precision-recall ------------------------------------------------------------\npr.svm &lt;- pr.curve(p.svm[which(val.set$mbd == \"MBD\")],\n                 p.svm[which(val.set$mbd == \"NoMBD\")],\n                 curve = TRUE)\npr.inr &lt;- pr.curve(scores.class0 = val.set$INR[which(val.set$mbd == \"MBD\"& !is.na(val.set$INR))],\n                 scores.class1 =  val.set$INR[which(val.set$mbd == \"NoMBD\"& !is.na(val.set$INR))],\n                 curve = TRUE)\npr.isth &lt;- pr.curve(scores.class0 = val.set$isth2[which(val.set$mbd == \"MBD\" & !is.na(val.set$isth2))],\n                 scores.class1 = val.set$isth2[which(val.set$mbd == \"NoMBD\" & !is.na(val.set$isth2))],\n                 curve = TRUE)\npr.pfa &lt;- pr.curve(scores.class0 = val.set$PFA_EPI[which(val.set$mbd == \"MBD\" & !is.na(val.set$PFA_EPI))],\n                      scores.class1 = val.set$PFA_EPI[which(val.set$mbd == \"NoMBD\"& !is.na(val.set$PFA_EPI))],\n                      curve = TRUE)\n\ncu.svm &lt;- pr.svm$curve\ncu.inr &lt;- pr.inr$curve\ncu.isth &lt;- pr.isth$curve\ncu.pfa &lt;- pr.pfa$curve\ncu.svm[,3] &lt;- \"MBD algorithm\"\ncu.inr[,3] &lt;- \"INR\"\ncu.isth[,3] &lt;- \"ISTH\"\ncu.pfa[,3] &lt;- \"PFA-EPI\"\ncu.comp &lt;- data.frame(rbind(cu.svm,cu.inr, cu.isth,cu.pfa))\ncu.comp$X1 &lt;- as.numeric(cu.comp$X1)\ncu.comp$X2 &lt;- as.numeric(cu.comp$X2)\ncu.comp$X3 &lt;- as.factor(cu.comp$X3)\n\n\npr.gg &lt;- ggplot(cu.comp, aes(x = X1, y= X2, group = factor(X3),colour =factor(X3) ))+ \n  geom_line(size = 1)+\n  scale_color_manual( values = c(\"#d6002a\",\"#666666\", \"#019970\",\"#5990b6\"))+\n  theme_pubr()+\n  labs(x=\"Sensitivity\",y=\"Positive predictive value\", title= paste(\"Precision-Recall Curve\"), color = \"Models\")+\n  xlim(0,.9999999)+\n  ylim(0,1)+\n  annotate(\"text\", x = 0.65, y = 0.15, size= 5, label = paste(\"INR AUC:\", round(pr.inr$auc.integral,2)), hjust = 0) +\n  annotate(\"text\", x = 0.65, y = 0.3, size= 5, label = paste(\"MBD Check AUC:\", round(pr.svm$auc.integral,2)), hjust = 0)+\n  annotate(\"text\", x = 0.65, y = 0.25, size= 5, label = paste(\"ISTH-BAT AUC:\", round(pr.isth$auc.integral,2)), hjust = 0)+\n  annotate(\"text\", x = 0.65, y = 0.2, size= 5, label = paste(\"PFA-EPI AUC:\", round(pr.pfa$auc.integral,2)), hjust = 0) +\n  coord_cartesian(expand = FALSE)\n\nrocble\npr.gg\n\n\n\n\n\n\n\n(a) ROC- Curve\n\n\n\n\n\n\n\n(b) PR-Curve\n\n\n\n\nFigure 4.1: Performance of the MBD-Check model in the external validation cohort compared to other diagnostic tests: (a) Receiver operating characteristic curve, (b) Precision recall curve. Abbreviations: AUC, area-under-the-curve; ISTH-BAT, bleeding assessment tool of the International Society on Thrombosis and Haemostasis; INR, international normalized ratio; PFA-EPI, platelet function analyzer with epinephrin-collagen cartridge."
  },
  {
    "objectID": "External_validation.html#calibration",
    "href": "External_validation.html#calibration",
    "title": "4  External validation",
    "section": "4.5 Calibration",
    "text": "4.5 Calibration\n\nggplot(calibration(relevel(val.set$mbd,2) ~ p.svm)) +\n  theme_pubr()\n\n\n\n\nFigure 4.2: Calibration curve of the model in the external validation set"
  },
  {
    "objectID": "IML.html#introduction",
    "href": "IML.html#introduction",
    "title": "5  IML",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn this document differnt interpretable machine learning techniques are used to understand how the MBD-Check algorithm makes predictions (Molnar, Casalicchio, and Bischl 2018)."
  },
  {
    "objectID": "IML.html#data-loading",
    "href": "IML.html#data-loading",
    "title": "5  IML",
    "section": "5.2 Data loading",
    "text": "5.2 Data loading\nThe trainings and testing data set are loaded\n\nlibrary(iml)\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(ggplot2)\nlibrary(missForest)\nlibrary(ggprism)\n\nble.tr &lt;- readRDS(\"TrainingsSet.RDS\")\nm.svm &lt;- readRDS(\"modelsvm.RDS\")\n\n\nset.seed(212312)\nble.tr.imp &lt;- missForest(ble.tr)$ximp"
  },
  {
    "objectID": "IML.html#permutation-feature-importance",
    "href": "IML.html#permutation-feature-importance",
    "title": "5  IML",
    "section": "5.3 Permutation feature importance",
    "text": "5.3 Permutation feature importance\nThis technique calculates the increase in prediction error when each feature is permuted to estimate the importance in the model.\n\npreds &lt;- c(\"mbd\",\"sex\",\"PFA_EPI\", \"surgery.simp\", \"menorrhagia.simp\" , \"postpartum.simp\" , \"minorwound.simp\" , \"epistaxis.simp\" ,  \"tooth.simp\"  , \"oralcavity.simp\",  \"aPTT\", \"cutaeous.simp\")\n\nset.seed(231231)\npredictor_iml &lt;- Predictor$new(m.svm, data = ble.tr.imp[preds],  type = \"prob\", y = \"mbd\")\nimp &lt;- FeatureImp$new(predictor_iml,loss = \"ce\",  compare = \"ratio\")\np1 &lt;- plot(imp)\np1 + \n  geom_vline(xintercept = 1, linetype = \"dashed\")+\n  theme_prism()+\n  scale_y_discrete(labels = rev(c(\"Simplified ISTH-BAT: Surgery\", \n                                  \"PFA-EPI\",\n                                  \"Simplified ISTH-BAT: Epistaxis\",\n                               \"Simplified ISTH-BAT: Menorrhagia\",\n                               \"Simplified ISTH-BAT: Tooth extraction\",\n                               \"aPTT\",\n                              \"Simplified ISTH-BAT: Cutaneous bleedings\", \n                              \"Simplified ISTH-BAT: Minor wounds\",\n                              \"Sex\",\n                              \"Simplified ISTH-BAT: Post-partum hemorrhage\",\n                               \"Simplified ISTH-BAT: Oral cavity\")))+\n  ylab(\"\") \n\n\n\n\nFigure 5.1: Interpretable machine learning: The importance of each feature included in the final model is illustrated. Permutation feature importance with cross-entropy as a loss function."
  },
  {
    "objectID": "IML.html#partial-dependence-plots-and-conditional-expectation-plots",
    "href": "IML.html#partial-dependence-plots-and-conditional-expectation-plots",
    "title": "5  IML",
    "section": "5.4 Partial dependence plots and conditional expectation plots",
    "text": "5.4 Partial dependence plots and conditional expectation plots\nThe Partial dependence plots (PDP) shows how much the predicted value changes when a variable of interest changes. It can show whether there is a linear or non-linear relationship between predicted values and the feature of interest. The conditional expectation plots (c-ICE) shows the change in the predicted value when all other variables are fixed.\n\npdp &lt;- FeatureEffects$new(predictor_iml,\n                          method = \"pdp+ice\")\nplot(pdp)\n\n\n\n\nFigure 5.2: Partial dependence plots and c-ICE plots showed an increase in prediction scores for all variables with higher levels, except for menorrhagia where emergency treatment or surgery resulted in lower prediction scores.\n\n\n\n\n\n5.4.1 Scaled PDP plot for aPTT and the PFA-EPI\n\npdp_aptt &lt;- FeatureEffects$new(predictor_iml,\n                          features = predictor_iml$model$coefnames[c(16)],\n                          center.at = 0.602,\n                          method = \"pdp+ice\")\nplot(pdp_aptt)\n\npdp_epi &lt;- FeatureEffects$new(predictor_iml,\n                               features = predictor_iml$model$coefnames[c(2)],\n                               center.at = 83,\n                               method = \"pdp+ice\")\nplot(pdp_epi)\n\n\n\n\n\n\n\n(a) aPTT\n\n\n\n\n\n\n\n(b) PFA-EPI\n\n\n\n\nFigure 5.3: C-ICE plots scaled on the lowest value in the data set."
  },
  {
    "objectID": "IML.html#accumulated-local-effects-plots-ale",
    "href": "IML.html#accumulated-local-effects-plots-ale",
    "title": "5  IML",
    "section": "5.5 Accumulated local effects plots (ALE)",
    "text": "5.5 Accumulated local effects plots (ALE)\nSimilar to the PDP, the ALE shows the change in predicted values but is unbiased by e.g. highly correlated variables\n\nale &lt;- FeatureEffects$new(predictor_iml,\n                          method = \"ale\")\nplot(ale) \n\n\n\n\nFigure 5.4: C-ICE plots centered on the lowest value in the data set.\n\n\n\n\n\n\n\n\nMolnar, Christoph, Giuseppe Casalicchio, and Bernd Bischl. 2018. “Iml: An R Package for Interpretable Machine Learning.” Journal of Open Source Software 3 (26): 786. https://doi.org/10.21105/joss.00786."
  },
  {
    "objectID": "Userevaluation.html#introduction",
    "href": "Userevaluation.html#introduction",
    "title": "6  User evaluation",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nIn this document the user evaluation is analyzed. Thirty-three surgeons, 29 anesthesiologists, and 24 hematologists participated in the survey. Twenty-four participants (27.9%) had 0-4 years of work experience, 16 (18.6%) had 5-9 years, 25 (29.1%) had 10-14 years, and 21 (24.4%) had 14+ years"
  },
  {
    "objectID": "Userevaluation.html#data-loading",
    "href": "Userevaluation.html#data-loading",
    "title": "6  User evaluation",
    "section": "6.2 Data loading",
    "text": "6.2 Data loading\n\nlibrary(ggplot2)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(ggbeeswarm)\nlibrary(tidyr)\nlibrary(hrbrthemes)\nlibrary(likert)\n\nres_nr &lt;- readRDS(\"answers1.RDS\")\nres2_num &lt;- readRDS(\"answers2.RDS\")"
  },
  {
    "objectID": "Userevaluation.html#time-needed-to-fill-out-the-case-vignettes",
    "href": "Userevaluation.html#time-needed-to-fill-out-the-case-vignettes",
    "title": "6  User evaluation",
    "section": "6.3 Time needed to fill out the case vignettes",
    "text": "6.3 Time needed to fill out the case vignettes\nParticipants were presented with 4 different case vignettes. The time needed to fill out and interpret each of the case vignette was automatically tracked in the survey platform.\n\n### By speciality\np1 &lt;- ggplot(res_nr[which(res_nr$time &lt;200),] ,aes(x=as.factor(spec), y=time, fill=as.factor(spec))) +\n  geom_boxplot(outlier.size = 0) +\n  scale_fill_manual(values = c(\"#FF6347\",\"#2E8B57\", \"#4169E1\"), labels = c(\"Surgeons\", \"Anesthesiologists\", \"Hematologists\")) +\n  geom_beeswarm(color=\"black\", size=0.8, alpha=0.9) +\n  theme_ipsum() +\n  theme(\n    legend.position=\"none\",\n    plot.title = element_text(size=11)\n  ) +\n  ggtitle(\"Time needed to fill out MBD Check by speciality\") +\n  stat_compare_means()+\n  xlab(\"\")+\n  scale_x_discrete(labels = c(\"Surgeons\", \"Anesthesiologists\", \"Hematologists\"))+\n  ylab(\"Time [s]\") \np1\n\n### By seniority\np2 &lt;- ggplot(res_nr[which(res_nr$time &lt;200),] ,aes(x=as.factor(seniority), y=time, fill=as.factor(seniority))) +\n  geom_boxplot(outlier.size = 0) +\n  scale_fill_manual(values = c(\"#FF6347\",\"#2E8B57\", \"#4169E1\", \"#FFA500\"), labels = c(\"0-4 Years\", \"5 - 9 Years\", \"10 - 14 Years\", \"&gt; 14 Years\")) +\n  geom_point(color=\"black\", size=0.8, alpha=0.9) +\n  theme_ipsum() +\n  theme(\n    legend.position=\"none\",\n    plot.title = element_text(size=11)\n  ) +\n  ggtitle(\"Time needed to fill out MBD Check by work experience\") +\n  scale_x_discrete(labels = c(\"0-4 Years\", \"5 - 9 Years\", \"10 - 14 Years\", \"&gt; 14 Years\"))+\n  stat_compare_means()+\n  xlab(\"\")+\n  ylab(\"Time [s]\") \np2\n\n\n\n\n\n\n\n(a) By speciality\n\n\n\n\n\n\n\n(b) By seniority\n\n\n\n\nFigure 6.1: Time needed to fill out the case vignettes."
  },
  {
    "objectID": "Userevaluation.html#system-usability-scale",
    "href": "Userevaluation.html#system-usability-scale",
    "title": "6  User evaluation",
    "section": "6.4 System usability scale",
    "text": "6.4 System usability scale\nParticipants were asked to fill out the system usability scale (SUS).\n\nlikert_df &lt;- res2_num[complete.cases(res2_num[,11:20]),c(1,22,11:20)]\ncolnames(likert_df) &lt;- c(\"Group\",\"Respondent\",\n                         \"Q01: I can see myself regularly using MBD-Check\",\n                         \"Q02: MBD-Check is unnecessarily complex\",\n                         \"Q03: MBD-Check is easy to use\",\n                         \"Q04: I think that I would need the support of a technical person to be able to use MBD-Check\",\n                         \"Q05: I found the various functions (Calculating and interpreting the proabbility of a MBD) of MBD-Check to be well implemented.\",\n                         \"Q06: I thought that there was too much inconsistency in MBD-Check\",\n                         \"Q07: I would imagine most people would learne to use MBD-Check very quickly\",\n                         \"Q08: I found MBD check very cumbersome to use\",\n                         \"Q09: I  felt very confident in using MBD-Check\",\n                         \"Q10: I needed to learn a lot of things before I could get going with MBD-Check\"\n                         )\nlikert_df$Group &lt;- factor(ifelse(likert_df$Group == 1, \"Surgeons\", \n                                    ifelse(likert_df$Group == 2, \"Anesthesiologists\", \"Hematologists\")), \n                          levels = c(\"Hematologists\", \"Anesthesiologists\", \"Surgeons\"))\n\nlbs &lt;- c(\"Disagree\", \"Somewhat disagree\", \"Neither agree nor disagree\",  \"Somewhat agree\", \"Agree\")\nsurvey &lt;- likert_df %&gt;%\n  dplyr::mutate_if(is.character, factor) %&gt;%\n  dplyr::mutate_if(is.numeric, factor, levels = 1:5, labels = lbs) %&gt;%\n  drop_na() %&gt;%\n  as.data.frame()\nplot(likert(survey[,3:12], grouping = survey[,1]), ordered = TRUE, wrap = 90) \n\n\n\n\nFigure 6.2: Answers to the system usability scale questions by speciality.\n\n\n\n\n\n6.4.1 SUS score\nBased on the answers the SUS score was calculated(Bangor, Kortum, and Miller 2008; Jordan et al. 1996). The median score was 82.5 and above the cut-off for excellent usability.\n\ncalculate_sus &lt;- function(x){\n  # Calculates the System Usability Score (SUS) from a vector \n  # Input: x = Vector with the sus; needs to include columns sus1 - sus10\n  # Output score = SUS\n  sus_scale &lt;- x[c(\"sus1\",\"sus2\",\"sus3\",\"sus4\",\"sus5\",\n                   \"sus6\", \"sus7\",\"sus8\",\"sus9\",\"sus10\")] -1\n  score &lt;- (sum(sus_scale[c(1,3,5,7,9)]-1)+sum(5-sus_scale[c(2,4,6,8,10)]))*2.5\n  return(score)\n}\n\nsus &lt;- unlist(lapply(1:nrow(res2_num), function(x){calculate_sus(res2_num[x,])}))\nres2_num$sus_g &lt;- sus\nsummary(sus)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  45.00   72.50   82.50   80.53   90.00  100.00      10 \n\nggplot(res2_num, aes(x = as.factor(spec), y = sus_g, color = seniority))+\n  geom_jitter(size = 3, height = 0, width = 0.3)+\n  geom_hline(yintercept = 68, linetype = \"dashed\")+\n  theme_pubr() +\n  theme(\n    plot.title = element_text(size=11),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  xlab(\"\")+\n  scale_x_discrete(labels = c(\"Surgeons\", \"Anesthesiologists\", \"Hematologists\"))+\n  ylab(\"System Usability Score\")+\n  ylim(c(0,100))+\n  scale_color_manual(values = c(\"#000000\",\"#666666\", \"#5cb14a\", \"#2E8B57\"), \n                     labels = c(\"0-4 Years\", \"5 - 9 Years\", \"10 - 14 Years\", \"&gt; 14 Years\"))+\n  labs(color = \"Years of work experience\")\n\n\n\n\n\n\n\n\nBangor, Aaron, Philip T. Kortum, and James T. Miller. 2008. “An Empirical Evaluation of the System Usability Scale.” International Journal of HumanComputer Interaction 24 (6): 574–94. https://doi.org/10.1080/10447310802205776.\n\n\nJordan, Patrick W., B. Thomas, Ian Lyall McClelland, and Bernard Weerdmeester. 1996. Usability Evaluation In Industry. CRC Press."
  }
]